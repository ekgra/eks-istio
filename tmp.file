
==> eks.tf <==
############################
# IAM for EKS control plane
############################
resource "aws_iam_role" "eks_cluster" {
  name               = "${var.cluster_name}-cluster-role"
  assume_role_policy = data.aws_iam_policy_document.eks_assume_role.json
}

data "aws_iam_policy_document" "eks_assume_role" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["eks.amazonaws.com"]
    }
  }
}

resource "aws_iam_role_policy_attachment" "eks_cluster_policy" {
  role       = aws_iam_role.eks_cluster.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
}

resource "aws_iam_role_policy_attachment" "eks_vpc_resource_controller" {
  role       = aws_iam_role.eks_cluster.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController"
}

############################
# EKS Cluster
############################
resource "aws_eks_cluster" "this" {
  name     = var.cluster_name
  role_arn = aws_iam_role.eks_cluster.arn
  version  = var.eks_version

  vpc_config {
    subnet_ids              = [for s in aws_subnet.public : s.id]
    endpoint_public_access  = true
    endpoint_private_access = false
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_vpc_resource_controller
  ]

  tags = {
    Name = var.cluster_name
  }
}

############################
# IAM for Node Group
############################
resource "aws_iam_role" "node" {
  name               = "${var.cluster_name}-node-role"
  assume_role_policy = data.aws_iam_policy_document.node_assume_role.json
}

data "aws_iam_policy_document" "node_assume_role" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
  }
}

resource "aws_iam_role_policy_attachment" "node_worker_policy" {
  role       = aws_iam_role.node.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}

resource "aws_iam_role_policy_attachment" "node_cni_policy" {
  role       = aws_iam_role.node.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

resource "aws_iam_role_policy_attachment" "node_ecr_ro" {
  role       = aws_iam_role.node.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}

############################
# Launch Template (to add public IP + extra SG)
############################
# EKS will automatically add its own managed SGs; this LT adds your extra SG too.
# Ensure associate_public_ip_address = true so nodes in public subnets get public IPs.
resource "aws_launch_template" "node_lt" {
  name_prefix = "${var.cluster_name}-lt-"

  network_interfaces {
    # Include ONLY your additional SG here; EKS will append the cluster-managed SGs.
    security_groups             = [aws_security_group.nodes_extra.id]
    associate_public_ip_address = true
  }

  # Keep it minimal; instance type is set on the node group.
  tag_specifications {
    resource_type = "instance"
    tags = {
      Name = "${var.cluster_name}-node"
    }
  }

  lifecycle {
    create_before_destroy = true
  }
}

############################
# Managed Node Group (Spot)
############################
resource "aws_eks_node_group" "spot_ng" {
  cluster_name    = aws_eks_cluster.this.name
  node_group_name = "${var.cluster_name}-ng-spot"
  node_role_arn   = aws_iam_role.node.arn
  subnet_ids      = [for s in aws_subnet.public : s.id]
  capacity_type   = "SPOT"

  scaling_config {
    desired_size = var.node_desired
    max_size     = var.node_max
    min_size     = var.node_min
  }

  instance_types = var.instance_types

  launch_template {
    id      = aws_launch_template.node_lt.id
    version = "$Latest"
  }

  update_config {
    max_unavailable = 1
  }

  tags = {
    Name = "${var.cluster_name}-ng-spot"
  }

  depends_on = [
    aws_iam_role_policy_attachment.node_worker_policy,
    aws_iam_role_policy_attachment.node_cni_policy,
    aws_iam_role_policy_attachment.node_ecr_ro
  ]
}

==> locals.tf <==
locals {
  # pick two AZs deterministically
  azs = slice(data.aws_availability_zones.available.names, 0, 2)

  # carve two /20 public subnets out of the /16
  public_subnets = {
    a = {
      az   = local.azs[0]
      cidr = cidrsubnet(var.vpc_cidr, 4, 0) # /20
    }
    b = {
      az   = local.azs[1]
      cidr = cidrsubnet(var.vpc_cidr, 4, 1) # /20
    }
  }
}
==> main.tf <==

==> network.tf <==
data "aws_availability_zones" "available" {
  state = "available"
}



resource "aws_vpc" "this" {
  cidr_block           = var.vpc_cidr
  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = {
    Name = "${var.cluster_name}-vpc"
  }
}

resource "aws_internet_gateway" "this" {
  vpc_id = aws_vpc.this.id

  tags = {
    Name = "${var.cluster_name}-igw"
  }
}

resource "aws_subnet" "public" {
  for_each = local.public_subnets

  vpc_id                  = aws_vpc.this.id
  availability_zone       = each.value.az
  cidr_block              = each.value.cidr
  map_public_ip_on_launch = true

  tags = {
    Name                                        = "${var.cluster_name}-public-${each.value.az}"
    "kubernetes.io/role/elb"                    = "1"
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
  }
}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.this.id

  tags = {
    Name = "${var.cluster_name}-public-rt"
  }
}

resource "aws_route" "public_inet" {
  route_table_id         = aws_route_table.public.id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = aws_internet_gateway.this.id
}

resource "aws_route_table_association" "public_assoc" {
  for_each       = aws_subnet.public
  subnet_id      = each.value.id
  route_table_id = aws_route_table.public.id
}

# Extra node security group to attach to the Node Group later.
# NLB (instance mode) forwards traffic to NodePorts; source IP is the client,
# so nodes must allow NodePort range (lock to your IP/32 for safer testing).
resource "aws_security_group" "nodes_extra" {
  name        = "${var.cluster_name}-nodes-extra"
  description = "Additional SG for nodes to allow NodePort from allowed_cidr"
  vpc_id      = aws_vpc.this.id

  egress {
    description = "all egress"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Kubernetes default NodePort range 30000-32767
  ingress {
    description = "NodePort range for NLB instance targets"
    from_port   = 30000
    to_port     = 32767
    protocol    = "tcp"
    cidr_blocks = [var.allowed_cidr]
  }

  tags = {
    Name = "${var.cluster_name}-nodes-extra"
  }
}

==> outputs.tf <==
output "vpc_id" {
  value = aws_vpc.this.id
}

output "public_subnet_ids" {
  value = [for s in aws_subnet.public : s.id]
}

output "public_subnet_cidrs" {
  value = { for k, s in aws_subnet.public : k => s.cidr_block }
}

output "public_route_table_id" {
  value = aws_route_table.public.id
}

output "nodes_additional_sg_id" {
  value       = aws_security_group.nodes_extra.id
  description = "Attach this SG to your EKS node group (vpc_security_group_ids) so NLB -> NodePort works."
}


# ========== EKS outputs ==========
output "cluster_name" {
  value = aws_eks_cluster.this.name
}

output "cluster_endpoint" {
  value = aws_eks_cluster.this.endpoint
}

output "cluster_certificate_authority" {
  value = aws_eks_cluster.this.certificate_authority[0].data
}

output "nodegroup_name" {
  value = aws_eks_node_group.spot_ng.node_group_name
}

==> provider.tf <==
provider "aws" {
  region = var.aws_region
}


==> variables.tf <==
variable "aws_region" {
  description = "AWS region to deploy into"
  type        = string
  default     = "ap-southeast-2"
}

variable "cluster_name" {
  description = "EKS cluster name (used for subnet tags)"
  type        = string
  default     = "demo-eks-istio"
}

variable "vpc_cidr" {
  description = "VPC CIDR"
  type        = string
  default     = "10.10.0.0/16"
}

variable "allowed_cidr" {
  description = "CIDR allowed to hit NodePort range on nodes (for NLB instance targets). Tighten to your IP/32 for security."
  type        = string
  default     = "0.0.0.0/0"
}


# ========== EKS variables ==========
variable "eks_version" {
  description = "EKS Kubernetes version"
  type        = string
  default     = "1.31"
}


variable "node_min" {
  type    = number
  default = 2
}

variable "node_desired" {
  type    = number
  default = 2
}
variable "node_max" {
  type    = number
  default = 4
}

variable "instance_types" {
  description = "Instance types for node group"
  type        = list(string)
  default     = ["t3.small"]
}



==> versions.tf <==
terraform {
  required_version = ">= 1.5.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}
